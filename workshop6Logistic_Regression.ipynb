{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandhya842/week1-AI-/blob/main/workshop6Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFKoS2dWFqTg"
      },
      "source": [
        "**# Classification-Introduction:**\n",
        "\n",
        "In Statistics and Machine Learning, classification is a type of supervised learning task, where training data with known class labels are given and are used to estimate a function, which can map a unseen  features (Idependent  Variables $X$) to its class label (Dependent Variable $Y$) .\n",
        "\n",
        "Tasks in Supervised Learning-Classification Task can be:\n",
        "  *   Binary Classification.\n",
        "  *   Multiclass Classification.\n",
        "\n",
        "This Notebook consist of One Sections:\n",
        "  *   Section-1: This section will discuss how can we use Logistic Regression to perform Binary Classification Task.\n",
        "    *   We also describe various helper function such as Sigmoid Function, Cost fucntion and disccuss.\n",
        "\n",
        "Author-Siman Giri."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IHoDWqv2pUkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSBFhL05IdaO"
      },
      "source": [
        "# Section-1: Binary Classification with Logistic Regression.\n",
        "\n",
        "___\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exqiG--qJKJh"
      },
      "source": [
        "## Helper Function:\n",
        "\n",
        "Let's discuss two function we need to further implement logistic regression.\n",
        "  1.   Sigmoid Function.\n",
        "  2.   Cost(Loss) Function.\n",
        "\n",
        "___\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sKX3VdDJsOk"
      },
      "source": [
        "### Sigmoid Function:\n",
        "A function  $[g : R -> R] $  is said to be a sigmoid function, if the function is bounded, differentiable, real function that is defined for all real input values and has a non-negative derivative at each point and exactly on inflection point. The sigmoid fucntion has a characterstic \"S\" shaped curved also known as sigmoid curve.  [--- Wikipedia]\n",
        "\n",
        "A comon example of a sigmoid function is the logistic function described as below:\n",
        "$$ g(x) = \\frac{1}{1+e^{-x}}, $$\n",
        "\n",
        "for $x \\in \\mathbb{R}$.\n",
        "\n",
        "The next two code blocks construct and plot this function.\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATSH37zsE0LX"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOXBvw9DNuyh"
      },
      "outputs": [],
      "source": [
        "def logistic_function(x):\n",
        "  \"\"\"\n",
        "  Computes the logistic function applied to any value of x.\n",
        "  Arguments:\n",
        "    x: scalar or numpy array of any size.\n",
        "  Returns:\n",
        "    y: logistic function applied to x.\n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  # Your Code here\n",
        "\n",
        "  y = 1 / (1 + np.exp(-x))\n",
        "\n",
        "  return y\n",
        "# Test Function:\n",
        "# For scalar:\n",
        "x = 0\n",
        "y = logistic_function(x)\n",
        "print(f\"logistic({x}) = {logistic_function(x)}\")\n",
        "# For Array:\n",
        "x_arr = np.array([-3, -1, 1, 3])\n",
        "y_arr = logistic_function(x_arr)\n",
        "print(f\"logistc({x_arr}) = {logistic_function(x_arr)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_GyQ3WHQPbJ"
      },
      "outputs": [],
      "source": [
        "# Plooting the sigmoid function:\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "plt.figure(figsize = (10, 5))\n",
        "x = np.linspace(-11, 11, 100)\n",
        "plt.plot(x, logistic_function(x), color = 'red')\n",
        "plt.xlabel(\"x\", fontsize = 14)\n",
        "plt.ylabel(\"g(x)\", fontsize = 14)\n",
        "plt.title(\"Standard logistic function\", fontsize = 14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6QM_2RWQsTw"
      },
      "source": [
        "### Cost(Loss) Function:\n",
        "Cost vs Loss Function:\n",
        "\n",
        "In general, loss function corresponds to observed error value between target value and predicted value for single observation/data points whereas cost function represents the average loss over a group of observations/data points.\n",
        "\n",
        "Due to the non-linearity introduced by logistic function in the function, we can not use squared loss as an loss function, instead we make use of log-loss function, given by:\n",
        "\n",
        "$$ L(y, y') = -y \\log\\left(y'\\right) - \\left(1 - y\\right) \\log\\left(1 - y'\\right), $$\n",
        "\n",
        "Where:\n",
        "*   $y$ : True target value (taking values $0$ or $1$)\n",
        "*   $y'$ : Predeicted target value ( predicted probability of $y$ being $1$ and vice versa.)\n",
        "\n",
        "The basis intution behind the log-loss function is, the loss value should be minimum when our predicted probability values are closer to true target value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDLxGVmwUQQ7"
      },
      "outputs": [],
      "source": [
        "def log_loss(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Computes log loss for true target value y ={0 or 1} and predicted target value y' inbetween {0-1}.\n",
        "  Arguments:\n",
        "    y_true (scalar): true target value {0 or 1}.\n",
        "    y_pred (scalar): predicted taget value {0-1}.\n",
        "  Returns:\n",
        "    loss (float): loss/error value\n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  loss = -y_true*np.log(y_pred)-(1-y_true)*np.log(1-y_pred)\n",
        "  return loss\n",
        "\n",
        "# Test function:\n",
        "y_true, y_pred = 0, 0.1\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')\n",
        "print(\"+++++++++++++--------------------------++++++++++++++++++++++++\")\n",
        "y_true, y_pred = 1, 0.9\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOWpQJFK30gr"
      },
      "outputs": [],
      "source": [
        "# Plot the loss Function:\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(1, 2, figsize = (15, 6), sharex = True, sharey = True)\n",
        "y_pred = np.linspace(0.0001, 0.9999, 100)\n",
        "ax[0].plot(y_pred, log_loss(0, y_pred), color = 'red')\n",
        "ax[0].set_title(\"y = 0\", fontsize = 14)\n",
        "ax[0].set_xlabel(\"y_pred\", fontsize = 14)\n",
        "ax[0].set_ylabel(\"log loss\", fontsize = 14)\n",
        "ax[1].plot(y_pred, log_loss(1, y_pred), color = 'red')\n",
        "ax[1].set_title(\"y = 1\", fontsize = 14)\n",
        "ax[1].set_xlabel(\"y_pred\", fontsize = 14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKRbJzqS4W9D"
      },
      "source": [
        "### Cost Function:\n",
        "As described above, we determine the cost function as an average of loss function value calculated for each observation/datapoints.\n",
        "\n",
        "Let $y = (y_1,....,y_n)$ be the true target values{(0 or 1)}  and $y'=(y_1',....y_n')$ be the corresponding predicted target values in between {(0-1)}, then the cost function be:\n",
        "\n",
        "$$ C(\\mathbf{y}, \\mathbf{y'}) = \\frac{1}{n}\\sum_{i = 1}^n L(y_i, y_i').\\tag{0} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "938aG1sz95b6"
      },
      "outputs": [],
      "source": [
        "# Cost function - using vectorization\n",
        "def cost_function(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes log loss for inputs true value (0 or 1) and predicted value (between 0 and 1)\n",
        "    Args:\n",
        "      y_true    (array_like, shape (m,)): array of true values (0 or 1)\n",
        "      y_pred (array_like, shape (m,)): array of predicted values (probability of y_pred being 1)\n",
        "    Returns:\n",
        "      cost (float): nonnegative cost corresponding to y_true and y_pred\n",
        "    \"\"\"\n",
        "    assert len(y_true) == len(y_pred), \"Length of true values and length of predicted values do not match\"\n",
        "    n = len(y_true)\n",
        "    loss_vec =-y_true*np.log(y_pred)-(1-y_true)*np.log(1-y_pred)\n",
        "    cost =np.sum(loss_vec)/n\n",
        "    return cost\n",
        "\n",
        "y_true, y_pred = np.array([0, 1, 0]), np.array([0.4, 0.6, 0.25])\n",
        "print(f\"cost_function({y_true}, {y_pred}) = {cost_function(y_true, y_pred)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--SYJgFONMZj"
      },
      "source": [
        "Extending the cost function for logistic regression to be used with model parameters.\n",
        "\n",
        "Function we are estimating:\n",
        "\n",
        "$$ y' = g\\left(\\mathbf{x} \\cdot \\mathbf{Î¸} + b\\right) = \\frac{1}{1 + e^{-\\left(\\mathbf{x} \\cdot \\mathbf{\\theta} + b\\right)}}. \\tag{1} $$\n",
        "\n",
        "Where:\n",
        "- $\\theta$ parameters (Coefficinet of feature variable) also known as weights.\n",
        "-$b$ parameters ( intercept of the function) also known as bias.\n",
        "\n",
        "Assume;\n",
        "- X: Matirix of n independent variables;\n",
        "- Y: Matrix of n dependent variables;\n",
        "- Y': Matrix of n predicted variables; and are represented as follows:\n",
        "\n",
        "$$ \\mathbf{X} = \\begin{pmatrix}\n",
        "x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\newline\n",
        "x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\newline\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\newline\n",
        "x_{m,1} & x_{m,2} & \\cdots & x_{m,n}\n",
        "\\end{pmatrix},\\;\\;\\;\\;\n",
        "\\mathbf{y} = \\begin{pmatrix}\n",
        "y_1 \\newline\n",
        "y_2 \\newline\n",
        "\\vdots \\newline\n",
        "y_m\n",
        "\\end{pmatrix},\\;\\;\\;\\;\n",
        "\\mathbf{y'} = \\begin{pmatrix}\n",
        "g\\left(\\mathbf{x_1} \\cdot \\mathbf{\\theta} + b\\right) \\newline\n",
        "g\\left(\\mathbf{x_2} \\cdot \\mathbf{\\theta} + b\\right) \\newline\n",
        "\\vdots \\newline\n",
        "g\\left(\\mathbf{x_n} \\cdot \\mathbf{\\theta} + b\\right)\n",
        "\\end{pmatrix}. \\tag{2} $$\n",
        "\n",
        "Now rewrite cost funtion defined in $(0)$ for model parameters as:\n",
        "\n",
        "$$ J\\left(\\mathbf{w}, b\\right) := C\\left(\\mathbf{y}, \\mathbf{y'} \\,\\vert\\, \\mathbf{X}, \\mathbf{w}, b \\right) = \\frac{1}{m}\\sum_{i = 1}^m L\\left(y_i, \\frac{1}{1 + e^{-\\left(\\mathbf{x_i} \\cdot \\mathbf{w} + b\\right)}}\\right) $$\n",
        "$$= \\frac{1}{m}\\sum_{i = 1}^m \\left[ -y_i \\log\\left(\\frac{1}{1 + e^{-\\left(\\mathbf{x_i} \\cdot \\mathbf{w} + b\\right)}}\\right) - \\left(1 - y_i\\right) \\log\\left(1 - \\frac{1}{1 + e^{-\\left(\\mathbf{x_i} \\cdot \\mathbf{w} + b\\right)}}\\right) \\right]. \\tag{3} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5k8Pi5r5ipE"
      },
      "outputs": [],
      "source": [
        "# Function to compute cost function in terms of model parameters - using vectorization\n",
        "def costfunction_logreg(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost function, given data and model parameters\n",
        "    Args:\n",
        "      X (ndarray, shape (m,n))  : data on features, m observations with n features\n",
        "      y (array_like, shape (m,)): array of true values of target (0 or 1)\n",
        "      w (array_like, shape (n,)): weight parameters of the model\n",
        "      b (float)                 : bias parameter of the model\n",
        "    Returns:\n",
        "      cost (float): nonnegative cost corresponding to y and y_dash\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    assert len(y) == m, \"Number of feature observations and number of target observations do not match\"\n",
        "    assert len(w) == n, \"Number of features and number of weight parameters do not match\"\n",
        "    z = b + np.dot(X, w)\n",
        "    y_pred = logistic_function(z)\n",
        "    cost = 1/m*np.sum(log_loss(y, y_pred))\n",
        "    return cost\n",
        "\n",
        "X, y, w, b = np.array([[10, 20], [-10, 10]]), np.array([1, 0]), np.array([0.5, 1.5]), 1\n",
        "print(f\"cost for logistic regression(X = {X}, y = {y}, w = {w}, b = {b}) = {costfunction_logreg(X, y, w, b)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAGw3-xqXJkW"
      },
      "source": [
        "### Gradient Descent Algorithm\n",
        "\n",
        "The gradient descent algorithm is a first order iterative optimization algorithm for finding alocal minimum of a differentiable function.\n",
        "\n",
        "The Algorithm:\n",
        "$$ \\begin{align*}\n",
        "& \\text{repeat until convergence:}\\; \\{ \\newline\n",
        "& w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j},\\; \\text{ for } j = 1, 2, \\ldots, n; \\newline\n",
        "& b := b -  \\alpha \\frac{\\partial J(\\mathbf{w}, b)}{\\partial b}. \\tag{4} \\newline\n",
        "& \\}\n",
        "\\end{align*} $$\n",
        "\n",
        "- where $\\alpha$ is the learning rate, and the parameters $\\mathbf{w} = (w_1, w_2, \\cdots, w_n)$ and $b$ are updated simultaniously in each iteration.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z9-J9sKX0Sd"
      },
      "source": [
        "**Compute Gradient:**\n",
        "\n",
        "Before we can implement the gradient descent algorithm, we need to compute the gradients, which are the partial derivatives of cost function $J(w,b)$ and are as follows:\n",
        "\n",
        "\n",
        "$$ \\begin{align*}\n",
        "& \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\frac{1}{m} \\sum\\limits_{i = 1}^m \\left(\\frac{1}{1 + e^{-\\left(\\mathbf{x_i} \\cdot \\mathbf{w} + b\\right)}} - y_i\\right) x_{i,j},\\;\\; \\text{ for } j = 1, 2, \\ldots, n; \\newline\n",
        "& \\frac{\\partial J(\\mathbf{w}, b)}{\\partial b} = \\frac{1}{m} \\sum\\limits_{i = 1}^m \\left(\\frac{1}{1 + e^{-\\left(\\mathbf{x_i} \\cdot \\mathbf{w} + b\\right)}} - y_i\\right).\n",
        "\\end{align*} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH6F9uAhYWlL"
      },
      "outputs": [],
      "source": [
        "# Function to compute gradients of the cost function with respect to model parameters - using vectorization\n",
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes gradients of the cost function with respect to model parameters\n",
        "    Args:\n",
        "      X (ndarray, shape (m,n))  : data on features, m observations with n features\n",
        "      y (array_like, shape (m,)): array of true values of target (0 or 1)\n",
        "      w (array_like, shape (n,)): weight parameters of the model\n",
        "      b (float)                 : bias parameter of the model\n",
        "    Returns:\n",
        "      grad_w (array_like, shape (n,)): gradients of the cost function with respect to the weight parameters\n",
        "      grad_b (float)                 : gradient of the cost function with respect to the bias parameter\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    assert len(y) == m, \"Number of feature observations and number of target observations do not match\"\n",
        "    assert len(w) == n, \"Number of features and number of weight parameters do not match\"\n",
        "    y_pred = logistic_function(b + np.dot(X, w))\n",
        "    grad_w = 1/m*np.dot(X.T, y_pred - y)\n",
        "    grad_b = 1/m*np.sum(y_pred - y)\n",
        "    return grad_w, grad_b\n",
        "\n",
        "X, y, w, b = np.array([[10, 20], [-10, 10]]), np.array([1, 0]), np.array([0.5, 1.5]), 1\n",
        "print(f\"gradient of logistic regression parameters (X = {X}, y = {y}, w = {w}, b = {b}) = {compute_gradient(X, y, w, b)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1KuJlZQYxJU"
      },
      "source": [
        "**Implement Gradient Descent for Logistic Regression.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd8IKGNIY4Dj"
      },
      "outputs": [],
      "source": [
        "# Gradient descent algorithm for logistic regression\n",
        "def gradient_descent(X, y, w, b, alpha, n_iter, show_cost = True, show_params = False):\n",
        "    \"\"\"\n",
        "    Implements batch gradient descent algorithm to learn and update model parameters\n",
        "    with prespecified number of interations and learning rate\n",
        "    Args:\n",
        "      X (ndarray, shape (m,n))  : data on features, m observations with n features\n",
        "      y (array_like, shape (m,)): true values of target (0 or 1)\n",
        "      w (array_like, shape (n,)): initial value of weight parameters\n",
        "      b (scalar)                : initial value of bias parameter\n",
        "      cost_func                 : function to compute cost\n",
        "      grad_func                 : function to compute gradients of cost with respect to model parameters\n",
        "      alpha (float)             : learning rate\n",
        "      n_iter (int)              : number of iterations\n",
        "    Returns:\n",
        "      w (array_like, shape (n,)): updated values of weight parameters\n",
        "      b (scalar)                : updated value of bias parameter\n",
        "    \"\"\"\n",
        "    from tqdm.contrib import itertools\n",
        "    import math\n",
        "    import tqdm\n",
        "    from time import sleep\n",
        "    m, n = X.shape\n",
        "    assert len(y) == m, \"Number of feature observations and number of target observations do not match\"\n",
        "    assert len(w) == n, \"Number of features and number of weight parameters do not match\"\n",
        "    cost_history, params_history = [], []\n",
        "    for i, j in itertools.product(range(n_iter), range(1)):\n",
        "        grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "        w += w - alpha * grad_w\n",
        "        b += b - alpha * grad_b\n",
        "        cost = costfunction_logreg(X, y, w, b)\n",
        "\n",
        "        cost_history.append(cost)\n",
        "        params_history.append([w, b])\n",
        "\n",
        "\n",
        "    return w, b, cost_history, params_history\n",
        "\n",
        "X, y, w, b, alpha, n_iter = np.array([[0.1, 0.2], [-0.1, 0.1]]), np.array([1, 0]), np.array([0., 0.]), 0., 0.1, 100000\n",
        "w_out, b_out, cost_history, params_history = gradient_descent(X, y, w, b, alpha, n_iter, show_cost = False, show_params = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI96yman53sw"
      },
      "outputs": [],
      "source": [
        "# Plotting cost over iteration\n",
        "plt.figure(figsize = (9, 6))\n",
        "plt.plot(cost_history)\n",
        "plt.xlabel(\"Iteration\", fontsize = 14)\n",
        "plt.ylabel(\"Cost\", fontsize = 14)\n",
        "plt.title(\"Cost vs Iteration\", fontsize = 14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXOAgvPxabDh"
      },
      "source": [
        "# Logistic Regression-Scratch Implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_aT7vJFaPwB"
      },
      "outputs": [],
      "source": [
        "path_dataset = \"/content/drive/MyDrive/AI/Iris.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CdYY2dYuIcr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwYRQb82vgh4"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(path_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USgZSg4KvkIo"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLM-ywy8vl7W"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_9e-pvYvoOG"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbakzEvA1HkS"
      },
      "outputs": [],
      "source": [
        "data = data.drop([\"Id\"], axis=1)\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkXqaIfWvq3m"
      },
      "outputs": [],
      "source": [
        "data.Species = data.Species.replace(data.Species.unique(),[*range(len(data.Species.unique()))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze1dC6Mqw-V7"
      },
      "outputs": [],
      "source": [
        "binary_iris = data[data.Species!=2].reset_index(drop=True)\n",
        "binary_iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7ZkU9oTxEZJ"
      },
      "outputs": [],
      "source": [
        "X = binary_iris.iloc[:, :-1].values\n",
        "Y = binary_iris.iloc[:,-1].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBolOOAg00UZ"
      },
      "outputs": [],
      "source": [
        "# Sanity Check:\n",
        "if X.shape[0] == Y.shape[0]:\n",
        "  print(\"Progress Further\")\n",
        "else:\n",
        "  print(\"X and Y are not created correctly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3l2CA0F1u_y"
      },
      "source": [
        "# Train Test Split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yPeV5Nu01lX"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-8WO_TX1XpJ"
      },
      "outputs": [],
      "source": [
        "# Sanity Check:\n",
        "if x_train.shape[0] == y_train.shape[0]:\n",
        "  print(\"Progress Further\")\n",
        "else:\n",
        "  print(\"x_train and y_train are not created correctly\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWC1GJTG6KGg"
      },
      "source": [
        "# Model Fitting:\n",
        "\n",
        "Training of the Model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDA8HhY82CPX"
      },
      "outputs": [],
      "source": [
        "# Initiialization:\n",
        "w_init = np.zeros(x_train.shape[1])\n",
        "b_init = -1\n",
        "# Learning model parameters using gradient descent algorithm\n",
        "w_out, b_out, cost_history, params_history = gradient_descent(x_train,\n",
        "                                                       y_train,\n",
        "                                                       w = w_init,\n",
        "                                                       b = b_init,\n",
        "                                                       alpha = 0.1,\n",
        "                                                       n_iter = 2000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnFlAjjI7B26"
      },
      "outputs": [],
      "source": [
        "# Plotting cost over iteration\n",
        "plt.figure(figsize = (9, 6))\n",
        "plt.plot(cost_history)\n",
        "plt.xlabel(\"Iteration\", fontsize = 14)\n",
        "plt.ylabel(\"Cost\", fontsize = 14)\n",
        "plt.title(\"Cost vs Iteration\", fontsize = 14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nt2tq4OIfRv"
      },
      "source": [
        "### **Prediction and Decision Boundry**\n",
        "- Prediction:\n",
        "\n",
        "  - Utilizing our trained weights and biases we first find the probabilities values y_probab for the x_test.\n",
        "Then, Y_probabiliy value is transformed to discrete class value using Decision boubdry.\n",
        "___\n",
        "- Decision Boundry:\n",
        "  - We first calculate the y_prediction as probabilities value, which is then converted to discrete class by using a threshold value. For instance, we take the threshold to be 0.5,then we classify the observation to class 1 and to class 0 otherwise.\n",
        "___\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVJoulDp7Ppt"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "def prediction(x_test, w_out, b_out):\n",
        "  \"\"\"\n",
        "  Computes the prediction for given test values.\n",
        "  Arguments: Inputs\n",
        "  x_test (nd array): Array of Test Independent variables.\n",
        "  w_out (nd array): Array of weights learned via gradient descent.\n",
        "  b_out (nd array) Array of bias learned via gradiebt descent.\n",
        "  Arguments: Output\n",
        "  y_pred (nd arrray): Array of Predicted dependent Variables.\n",
        "  \"\"\"\n",
        "  y_test_prob = logistic_function(np.matmul(x_test, w_out) + (b_out * np.ones(x_test.shape[0])))\n",
        "  y_pred = []\n",
        "  # Your code here\n",
        "\n",
        "    # Convert probabilities to class labels (0 or 1)\n",
        "  y_pred = (y_test_prob > 0.5).astype(int)\n",
        "\n",
        "\n",
        "  return y_pred\n",
        "# y_pred = (y_test_prob > 0.5).astype(int)\n",
        "\n",
        "y_pred = prediction(x_test, w_out, b_out)\n",
        "y_pred = np.array(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV9A8zUSIPtG"
      },
      "outputs": [],
      "source": [
        "y_pred.shape == y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItzfMfRqIVNO"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U94YcuS5IXea"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy_j54t7IN8R"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.imshow(cm)\n",
        "ax.grid(False)\n",
        "ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
        "ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
        "ax.set_ylim(1.5, -0.5)\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax.text(j, i, cm[i, j], ha='center', va='center', color='white')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5Ddb0QLEV90-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs5VFUEhFnEM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Home-Work {Graded Task - Deadline - Next-workshop}\n",
        "With the help of Sklearn Library Implement a Multiclass Classification for any data of your choices.\n",
        "\n"
      ],
      "metadata": {
        "id": "EwoCzlwc2l39"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1kTxqpI92oWl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}